{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1eHt41ampHutDCFclqChTNh8ORgIYWa37",
      "authorship_tag": "ABX9TyMmiGyZwkR+WtEUJ6X7vth7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parhamvz73/Machine-Learning/blob/main/Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Project Overview & Problem Definition"
      ],
      "metadata": {
        "id": "Zo035f0sVmWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Starting With This**\n",
        "\n",
        "Before I jump into coding, cleaning data, or building models, I want to clearly understand the problem I’m solving.\n",
        "If I don’t do this properly:\n",
        "\n",
        "1.  might waste time exploring irrelevant aspects of the data.\n",
        "\n",
        "2. I won’t know how to measure whether my model is “good enough.”\n",
        "\n",
        "3. I might accidentally draw wrong conclusions because I didn’t think about assumptions and limitations.\n",
        "\n",
        ">From my perspective, a well-defined problem statement is the foundation of any successful data science project."
      ],
      "metadata": {
        "id": "hOESOEIDXfxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Project Title"
      ],
      "metadata": {
        "id": "9b4dYnHyXyen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I always start with a **simple** but **descriptive** project title.\n",
        "\n",
        "Weak title: ***\"Titanic Dataset\"***\n",
        "\n",
        "My title: ***\"Predicting Survival on the Titanic (Binary Classification Project)\"***\n",
        "\n",
        "This way, anyone reading my notebook will immediately know:\n",
        "\n",
        "1. What the project is about\n",
        "\n",
        "2. What type of machine learning task I am working on (classification)"
      ],
      "metadata": {
        "id": "8Gxpckj-Yj8t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HdO33EOwY1gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Background / Context"
      ],
      "metadata": {
        "id": "bQgs6B3tY79d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I describe the story behind the dataset and why it matters to me.\n",
        "\n",
        "I always start by asking myself a few key questions:\n",
        "\n",
        "- Where does the dataset come from?\n",
        "\n",
        "- What type of information does it contain?\n",
        "\n",
        "- Why is this problem important or valuable to solve?\n",
        "\n",
        ">The dataset I am working with contains records of individuals, items, or events, along with several descriptive features. The data is intended to support the prediction or classification of an outcome variable.\n",
        "\n",
        "Why is this important for me?\n",
        "\n",
        "- It is a commonly used dataset for practicing machine learning and gives me a safe environment to improve my workflow.\n",
        "\n",
        "- It is also inspired by real-world problems, where social, demographic, business, or environmental factors have a significant impact on outcomes."
      ],
      "metadata": {
        "id": "1gZrxBkeY_Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MP9UHkBoZ4OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement"
      ],
      "metadata": {
        "id": "izTnWtXpZ5QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to keep this short and precise.\n",
        "\n",
        "- **Input:** Features or attributes available in the dataset (e.g., numerical, categorical, or text-based variables).\n",
        "\n",
        "- **Output:** Target outcome (e.g., a binary label, a continuous value, or a category).\n",
        "\n",
        "**My problem statement:**\n",
        "\n",
        ">The goal of my project is to predict the target outcome based on the available descriptive and contextual features in the dataset."
      ],
      "metadata": {
        "id": "c8VAJHtrZ7hW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e78sOfm0aDuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Goals & Objectives"
      ],
      "metadata": {
        "id": "VJQbL7XHaD55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I split my goals into primary and secondary to stay organized.\n",
        "\n",
        "**Primary Goal:**\n",
        "\n",
        "- Build a machine learning model that predicts the target variable with at least a predefined performance threshold (e.g., accuracy above 80%).\n",
        "\n",
        "**Secondary Goals:**\n",
        "\n",
        "- Perform exploratory data analysis (EDA) to discover meaningful patterns.\n",
        "\n",
        "- Visualize which groups or categories show significant differences in outcomes.\n",
        "\n",
        "- Identify the most important predictors or drivers of the target variable.\n",
        "\n",
        "- Document assumptions, challenges, and limitations clearly."
      ],
      "metadata": {
        "id": "Wev3E5UmaHyx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSX1ARzuasdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Success Criteria & Evaluation Metrics"
      ],
      "metadata": {
        "id": "wpczyVCWauBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For me, success means having a measurable metric that I can track.\n",
        "\n",
        "Depending on the project type, I might use:\n",
        "\n",
        "- **Classification problems:** ***Accuracy, Precision, Recall, F1-score, ROC-AUC***\n",
        "\n",
        "- **Regression problems:** ***MSE, RMSE, MAE, R²***\n",
        "\n",
        "Since evaluation criteria often depend on the project context, I will choose one primary metric and track it consistently throughout the project.\n",
        "\n",
        "| Feature   | Description     | Example |  \n",
        "|-----------|----------------|---------|  \n",
        "| Classification      | Accuracy, Precision, Recall, AUC | Accuracy (example) |  \n",
        "| Regression       | MSE, RMSE, R²   | RMSE (example)  |  \n"
      ],
      "metadata": {
        "id": "XKhDyUIdaxbu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLw5PiY0bVsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assumptions & Limitations"
      ],
      "metadata": {
        "id": "D8OAaJIqbV6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to be honest about what I assume and what might limit my work.\n",
        "\n",
        "- **My Assumptions:**\n",
        "\n",
        "  - The dataset is representative of the real-world scenario.\n",
        "\n",
        "   - Missing values can be imputed without introducing heavy bias.\n",
        "\n",
        "    - The provided features are sufficient to train a predictive model.\n",
        "\n",
        "- **My Limitations:**\n",
        "\n",
        "   - The dataset may be relatively small or imbalanced.\n",
        "\n",
        "   - Some variables may contain too many missing values to be useful.\n",
        "\n",
        "   - Historical, demographic, or business biases may affect predictions.\n",
        "\n",
        ">⚠️ By writing this down, I remind myself (and anyone reading) not to over-interpret the results."
      ],
      "metadata": {
        "id": "bNIQpKXTbYC7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0dDTI3yb40o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##My Project Checklist"
      ],
      "metadata": {
        "id": "gNBrEMYNb6v3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I use a simple checklist to stay organized:\n",
        "\n",
        "- ✅ Define project title\n",
        "\n",
        "- ✅ Write problem statement\n",
        "\n",
        "- ⬜️ Explore dataset source and size\n",
        "\n",
        "- ⬜️ Identify target variable\n",
        "\n",
        "- ⬜️ Choose evaluation metric\n",
        "\n",
        "- ⬜️ Document assumptions and limitations"
      ],
      "metadata": {
        "id": "yOamd1zXb9cp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4F3v8oZcUqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Dictionary & Schema"
      ],
      "metadata": {
        "id": "8A30ldmxcl58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I describe the structure of my dataset and document each column so I have a clear reference throughout the project.\n",
        "\n",
        "I always start by asking myself a few key questions:\n",
        "\n",
        "- What columns exist in the dataset?\n",
        "\n",
        "- What type of values do they contain (numeric, categorical, text, date)?\n",
        "\n",
        "- Which ones are identifiers, features, targets, or metadata?\n",
        "\n",
        "- How much missing data do I need to account for?\n",
        "\n",
        ">A well-written data dictionary helps me avoid confusion later, ensures I handle missing values correctly, and gives me a map for cleaning, encoding, and modeling."
      ],
      "metadata": {
        "id": "ibMuMnrfkkdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Schema Overview (Template)"
      ],
      "metadata": {
        "id": "9NzkJ7qZcqQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I create a table that summarizes each column.\n",
        "\n",
        "| Column Name | Role (ID / Target / Feature / Meta) | Data Type | Unit / Format | Allowed Values / Range | Missing % | Description                                            |\n",
        "| ----------- | ----------------------------------- | --------- | ------------- | ---------------------- | --------- | ------------------------------------------------------ |\n",
        "| id          | ID                                  | integer   | unique id     | positive integers      | 0%        | Unique identifier per row                              |\n",
        "| target      | Target                              | int (0/1) | binary        | {0,1} or {yes,no}      | 0%        | The outcome variable I want to predict                 |\n",
        "| feature\\_1  | Feature                             | float     | numeric       | ≥0                     | 5%        | Continuous variable representing a measurable property |\n",
        "| feature\\_2  | Feature                             | category  | string        | {A, B, C, D}           | 0%        | Categorical variable with limited values               |\n",
        "| feature\\_3  | Feature                             | datetime  | YYYY-MM-DD    | valid date range       | 2%        | Date or time-related variable                          |\n",
        "| notes       | Meta                                | text      | free string   | n/a                    | 10%       | Optional comments or additional info                   |\n"
      ],
      "metadata": {
        "id": "Oh2nMPykkwX5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VmrTieg8lBbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Field-by-Field Notes"
      ],
      "metadata": {
        "id": "EyIpOG2elBvO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes a table is not enough. For important variables, I write a short explanation:\n",
        "\n",
        "- **Target Variable:**\n",
        "\n",
        ">This is the label I am trying to predict. It is only present in the training set and absent in the test set. I also check its distribution to see if it is balanced or imbalanced.\n",
        "\n",
        "- **Identifiers:**\n",
        "\n",
        ">Unique IDs are useful for joining or submissions but not included in the model.\n",
        "\n",
        "- **Categorical Features:**\n",
        "\n",
        ">I note all distinct categories and check if rare levels exist that should be grouped into “Other.”\n",
        "\n",
        "- **Datetime Features:**\n",
        "\n",
        ">For date fields, I record the format, timezone, and coverage period. Later I might extract useful components like year, month, or weekday.\n",
        "\n",
        "- **Numeric Features:**\n",
        "\n",
        ">I record valid ranges and units. If there are impossible values (e.g., negatives where not expected), I log them for correction."
      ],
      "metadata": {
        "id": "vXH5a7H0lDS6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKp_9cfclRTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missingness Audit"
      ],
      "metadata": {
        "id": "lCdA95VGlRb3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I check how many missing values exist in each column and plan how to handle them.\n",
        "\n",
        "| Column     | Missing % | Possible Reason    | Imputation Plan                  |\n",
        "| ---------- | --------- | ------------------ | -------------------------------- |\n",
        "| feature\\_1 | 5%        | data not recorded  | fill with median or group median |\n",
        "| feature\\_3 | 2%        | occasional errors  | forward fill / interpolation     |\n",
        "| notes      | 10%       | optional free text | ignore for modeling              |\n"
      ],
      "metadata": {
        "id": "-TZvSsb5lVmk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0cNNrz8lcAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Levels & Encoding Plan"
      ],
      "metadata": {
        "id": "0y8AUCnRlcHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each categorical feature, I plan how I will encode it:\n",
        "\n",
        "- feature_2: 4 levels {A, B, C, D} → one-hot encoding\n",
        "\n",
        "- feature_city: 200+ levels → group rare categories into “Other,” then one-hot encode\n",
        "\n",
        "- feature_quality: ordinal {low, medium, high} → label encoding with order"
      ],
      "metadata": {
        "id": "37rXNmkplfs_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "014SKMNMlf5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Planned Derived Features"
      ],
      "metadata": {
        "id": "a1IZo4K8lnHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also note any new features I may create later:\n",
        "\n",
        "`feature_ratio = feature_a / feature_b`\n",
        "\n",
        "`days_since_event = current_date - feature_3`\n",
        "\n",
        "`is_missing_flag = 1 if feature_1 is missing, else 0`"
      ],
      "metadata": {
        "id": "3cPWNZ-Qloz7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVtZm50el0uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Data Dictionary Checklist"
      ],
      "metadata": {
        "id": "doE7_3CKl02v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ✅ I listed all columns with descriptions\n",
        "\n",
        "- ✅ I defined data types and valid ranges\n",
        "\n",
        "- ⬜️ I recorded missingness per column\n",
        "\n",
        "- ⬜️ I assigned roles (ID, Target, Feature, Meta)\n",
        "\n",
        "- ⬜️ I drafted encoding and imputation strategies\n",
        "\n",
        "- ⬜️ I logged potential derived features"
      ],
      "metadata": {
        "id": "2Hh0fO5Ql4PH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFHJgJSwmFJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Dataset Overview & Initial Inspection (EDA-0)"
      ],
      "metadata": {
        "id": "N7PElkH1mFSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why I Am Doing This\n",
        "\n",
        "Before I clean, transform, or model anything, I want to get familiar with the dataset at a high level.\n",
        "This is like taking a first walk through the data:\n",
        "\n",
        "- How many rows and columns are there?\n",
        "\n",
        "- What types of variables am I dealing with?\n",
        "\n",
        "- How balanced is the target variable?\n",
        "\n",
        "- Do I notice any immediate problems (missing values, duplicates, strange outliers)?\n",
        "\n",
        ">The goal here is not deep analysis yet — just basic orientation so I know what I’m working with."
      ],
      "metadata": {
        "id": "X-kGB48zmXoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Snapshot"
      ],
      "metadata": {
        "id": "zfO8C8oMmTuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing I check is the basic shape and structure of the dataset.\n",
        "\n",
        "- Number of rows: total observations (how many examples I have)\n",
        "\n",
        "- Number of columns: total features (how many variables I can work with)\n",
        "\n",
        "- Granularity: what each row represents (an individual, a transaction, a product, a time series point, etc.)\n",
        "\n",
        "- Files / splits: do I have train.csv / test.csv, or just one dataset to split myself?\n",
        "\n",
        "I also want to confirm if the dataset is small, medium, or large, since that affects how I’ll handle computation."
      ],
      "metadata": {
        "id": "3iw-kykRmrBE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9i_mB25ym41r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Types & Structure"
      ],
      "metadata": {
        "id": "B_FSHkstm5ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then review the types of variables:\n",
        "\n",
        "- Numeric (continuous / discrete): e.g., age, income, counts\n",
        "\n",
        "- Categorical (nominal / ordinal): e.g., gender, class, quality rating\n",
        "\n",
        "- Datetime / temporal: e.g., order date, timestamp\n",
        "\n",
        "- Text / free-form: e.g., comments, names, reviews\n",
        "\n",
        "- Identifiers / keys: unique IDs, transaction numbers\n",
        "\n",
        ">This helps me plan how I’ll encode variables later (scaling for numbers, one-hot encoding for categories, extracting components for dates, etc.)."
      ],
      "metadata": {
        "id": "lGQEBmiGm8_6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wIJ6IPbnCkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Target Variable (for Supervised Projects)"
      ],
      "metadata": {
        "id": "B8yQUWpwnCqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If my project is supervised (classification or regression), I look closely at the target column:\n",
        "\n",
        "- Is the target present only in training data and not in test?\n",
        "\n",
        "- How many unique values does it have (binary, multi-class, continuous)?\n",
        "\n",
        "- What is the distribution (balanced or imbalanced)?\n",
        "\n",
        "| Target Value | Count | Percentage |\n",
        "| ------------ | ----- | ---------- |\n",
        "| Class 0      | …     | … %        |\n",
        "| Class 1      | …     | … %        |\n",
        "| **Total**    | …     | 100%       |\n",
        "\n",
        ">If I find imbalance (e.g., 90% vs 10%), I know I’ll need to use metrics like F1-score, ROC-AUC, or balanced accuracy instead of plain accuracy."
      ],
      "metadata": {
        "id": "WXcSh-gEnPky"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVt6UvoQnXq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values Overview"
      ],
      "metadata": {
        "id": "ZvQbA4-ynXxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this stage, I don’t fix missing values yet — I just record them.\n",
        "\n",
        "- Which columns have missing values?\n",
        "\n",
        "- What percentage of the data is missing in each column?\n",
        "\n",
        "- Does missingness look random, or is it tied to specific conditions?\n",
        "\n",
        "| Column     | Missing % | Notes                     |\n",
        "| ---------- | --------- | ------------------------- |\n",
        "| feature\\_1 | 5%        | likely missing at random  |\n",
        "| feature\\_2 | 0%        | complete                  |\n",
        "| feature\\_3 | 20%       | might depend on subgroups |\n"
      ],
      "metadata": {
        "id": "Cww33DJqng7P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1SCsRIgng_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Descriptive Stats"
      ],
      "metadata": {
        "id": "zJYyr8Q8npur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I generate basic descriptive statistics to get a sense of the data:\n",
        "\n",
        "- For numeric columns: mean, median, min, max, standard deviation\n",
        "\n",
        "- For categorical columns: number of unique values, most common categories\n",
        "\n",
        "- For datetime columns: range of dates, earliest/latest record\n",
        "\n",
        "This gives me early warnings of:\n",
        "\n",
        "- Unrealistic values (e.g., negative ages, impossible dates)\n",
        "\n",
        "- Very high cardinality (e.g., 10,000 unique categories for a “city” column)\n",
        "\n",
        "- Potential outliers"
      ],
      "metadata": {
        "id": "ojZ66RtensAk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pv7u6X93n6Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicates & Keys"
      ],
      "metadata": {
        "id": "OBWWSUxOn6iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I check whether:\n",
        "\n",
        "- Each row is unique (based on the supposed key column).\n",
        "\n",
        "- There are any duplicate rows or IDs.\n",
        "\n",
        "- Keys or identifiers are truly unique — if not, I log this for cleaning later."
      ],
      "metadata": {
        "id": "FSRf-zfnoA_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wyd59gtjoElf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Impressions & Notes"
      ],
      "metadata": {
        "id": "Ibny6ZIioEpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the end of this inspection, I write down my initial thoughts:\n",
        "\n",
        "- What seems straightforward and ready to use?\n",
        "\n",
        "- Which features look suspicious or noisy?\n",
        "\n",
        "- Which areas need deeper exploration in the next step (EDA-1)?"
      ],
      "metadata": {
        "id": "5EYd8mgtoIWP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "21NbP1zFoMYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Initial Inspection Checklist"
      ],
      "metadata": {
        "id": "QfuNhsO4oMdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Checked dataset shape (rows, columns)\n",
        "\n",
        " - ✅ Confirmed what each row represents (granularity)\n",
        "\n",
        " - ⬜️ Reviewed variable types (numeric, categorical, datetime, text, ID)\n",
        "\n",
        " - ⬜️ Inspected target variable distribution (if applicable)\n",
        "\n",
        " - ⬜️ Logged missing values per column\n",
        "\n",
        " - ⬜️ Reviewed descriptive statistics\n",
        "\n",
        " - ⬜️ Checked for duplicates and unique IDs\n",
        "\n",
        " - ⬜️ Wrote down first impressions"
      ],
      "metadata": {
        "id": "x-yDu2vToQTD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vIg2PZBpooty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Exploratory Data Analysis (EDA-1)"
      ],
      "metadata": {
        "id": "bpxPVF6poo1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "Now that I know the basic structure of my dataset, I want to explore it in more depth.\n",
        "The purpose of this step is not yet to build models, but to:\n",
        "\n",
        "- Understand the distribution of variables.\n",
        "\n",
        "- Detect patterns, correlations, and group differences.\n",
        "\n",
        "- Spot outliers, anomalies, or data quality issues.\n",
        "\n",
        "- Generate hypotheses about what features may matter for prediction.\n",
        "\n",
        ">EDA is about asking questions like: “What influences the target? Are there clear groups or trends? What features interact with each other?”"
      ],
      "metadata": {
        "id": "_6KsLT6-pDWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Univariate Analysis"
      ],
      "metadata": {
        "id": "e9D5g3G4pJOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I start with one variable at a time:\n",
        "\n",
        "- Numeric features: check histograms, boxplots, and descriptive statistics.\n",
        "\n",
        "   - Are they normally distributed or skewed?\n",
        "\n",
        "   - Do they have extreme values?\n",
        "\n",
        "   - Are there obvious data entry errors?\n",
        "\n",
        "- Categorical features: check frequency counts and bar charts.\n",
        "\n",
        "   - Are some categories dominant?\n",
        "\n",
        "   - Do I have rare categories that should be grouped into “Other”?\n",
        "\n",
        "   - Is the distribution balanced or highly imbalanced?\n",
        "\n",
        "- Datetime features:\n",
        "\n",
        "   - Do I have seasonal trends?\n",
        "\n",
        "   - Is there missing coverage for certain time periods?"
      ],
      "metadata": {
        "id": "bBcgOn8GpR6f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z93jkW_KphnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bivariate Analysis (Feature vs Target)"
      ],
      "metadata": {
        "id": "4C4ub3P6pJTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then explore how each feature relates to the target variable.\n",
        "\n",
        "- For numeric vs target (classification): compare means/medians across target groups, visualize with boxplots or violin plots.\n",
        "\n",
        "- For categorical vs target: cross-tabulations and survival/response rates per category.\n",
        "\n",
        "- For regression problems: scatter plots and correlation with the target.\n",
        "\n",
        "Example insight (generic):\n",
        "\n",
        ">Customers in category A may have twice the probability of a positive outcome compared to category B."
      ],
      "metadata": {
        "id": "Ezw5ObcsrJHt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NRoiJtfArOLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multivariate Analysis (Feature Interactions)"
      ],
      "metadata": {
        "id": "Qz2AFR66rOTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some insights only appear when looking at multiple variables together:\n",
        "\n",
        "- Numeric vs numeric (scatter plots, correlation heatmaps).\n",
        "\n",
        "- Categorical vs categorical (stacked bar charts, grouped proportions).\n",
        "\n",
        "- Mixed feature interactions (e.g., does feature A matter differently depending on feature B?).\n",
        "\n",
        ">This helps me identify synergies or collinearity between variables."
      ],
      "metadata": {
        "id": "ewQoZA5CrVzW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LkNC5dArYdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation & Redundancy Check"
      ],
      "metadata": {
        "id": "cjVAF9yXrYiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For numeric variables, I check correlations:\n",
        "\n",
        "- High correlation (e.g., >0.9): indicates redundancy, I may drop one later.\n",
        "\n",
        "- Low correlation with target: doesn’t mean the feature is useless, but it sets expectations.\n",
        "\n",
        "- Multicollinearity: if many variables are correlated, I note this for modeling (especially linear models)."
      ],
      "metadata": {
        "id": "VcRMr4rUrbkw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "prgH29ylrly1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outliers & Anomalies"
      ],
      "metadata": {
        "id": "pny9pP4-rl7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I look for unusual cases that may distort models:\n",
        "\n",
        "- Extreme numeric values (e.g., income = 1e9).\n",
        "\n",
        "- Invalid categories (e.g., “???” or misspellings).\n",
        "\n",
        "- Dates far outside expected ranges.\n",
        "\n",
        "My decision:\n",
        "\n",
        "- Keep them (if real but rare events).\n",
        "\n",
        "- Transform them (e.g., log scale).\n",
        "\n",
        "- Remove them (if clear errors)."
      ],
      "metadata": {
        "id": "qr2wi5g4r_CU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQOUR0ZBsHqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Hypotheses"
      ],
      "metadata": {
        "id": "zSE6oewnsHwl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on EDA, I start forming early hypotheses about which features matter most.\n",
        "\n",
        "- Which features seem strongly linked to the target?\n",
        "\n",
        "- Which categories show big differences in outcome rates?\n",
        "\n",
        "- Which features appear noisy or irrelevant?\n",
        "\n",
        "I write these down so I can later compare my intuition vs actual model results."
      ],
      "metadata": {
        "id": "W7ZE_zOnsJ6c"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCxclDs9sO_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My EDA Checklist"
      ],
      "metadata": {
        "id": "mqKh0msTsPFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Reviewed distributions for all numeric features\n",
        "\n",
        " - ✅ Checked frequency tables for categorical features\n",
        "\n",
        " - ⬜️ Compared features against the target variable\n",
        "\n",
        " - ⬜️ Explored multivariate patterns and interactions\n",
        "\n",
        " - ⬜️ Logged correlations and possible redundancies\n",
        "\n",
        " - ⬜️ Investigated outliers and anomalies\n",
        "\n",
        " - ⬜️ Wrote down initial hypotheses"
      ],
      "metadata": {
        "id": "s_vvw4NSsRjj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_-g_ZwasbjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Data Cleaning & Preprocessing"
      ],
      "metadata": {
        "id": "nrS96TEFsc3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "After exploring the dataset, I now need to make it consistent, reliable, and usable for machine learning.\n",
        "Even the best model will fail if the input data is messy.\n",
        "\n",
        "The purpose of this step is to:\n",
        "\n",
        "- Handle missing values\n",
        "\n",
        "- Fix data type issues\n",
        "\n",
        "- Resolve duplicates\n",
        "\n",
        "- Correct or transform outliers\n",
        "\n",
        "- Standardize formats (dates, text, categories)\n",
        "\n",
        "- Ensure there is no data leakage\n",
        "\n",
        ">I think of this step as building a “clean kitchen” before cooking: I want my ingredients (data) organized and ready."
      ],
      "metadata": {
        "id": "pMzpCtxksrTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values"
      ],
      "metadata": {
        "id": "f5_FBPQvsyur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first check where data is missing and decide how to treat it:\n",
        "\n",
        "- Drop rows or columns (only if missingness is very high and uninformative).\n",
        "\n",
        "- Fill with statistical values:\n",
        "\n",
        "   - Numeric → mean, median, or group-based median\n",
        "\n",
        "   - Categorical → mode (most frequent value)\n",
        "\n",
        "- Use domain-specific rules (e.g., missing = “Unknown” or “Not applicable”).\n",
        "\n",
        "- Create missingness indicators (binary flags for whether data was missing).\n",
        "\n",
        ">I remind myself: imputing is never perfect — I choose a strategy that balances simplicity with accuracy."
      ],
      "metadata": {
        "id": "qGWhi9_qs1Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFvn1J6Ts9Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Type Corrections"
      ],
      "metadata": {
        "id": "tG7asol0s9nI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "make sure each column has the correct type:\n",
        "\n",
        "- IDs → integer or string, not float\n",
        "\n",
        "- Dates → converted to proper datetime objects\n",
        "\n",
        "- Categories → set as categorical variables\n",
        "\n",
        "- Numeric columns → checked for parsing errors (e.g., “1,000” stored as string)"
      ],
      "metadata": {
        "id": "n5p42NYQtHvY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1fHfLhL3tNx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing Duplicates"
      ],
      "metadata": {
        "id": "fJWwjYYUtN3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I check if:\n",
        "\n",
        "- Any rows are exact duplicates → remove them.\n",
        "\n",
        "- Keys (like ID) are duplicated → investigate why (data error or valid multi-records)."
      ],
      "metadata": {
        "id": "19BVG2c5tQGy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aK0GcVzqtSgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Outliers"
      ],
      "metadata": {
        "id": "l403OyiDtSki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers can distort models, so I decide whether to:\n",
        "\n",
        "- Keep them (if they are valid but rare events).\n",
        "\n",
        "- Cap them (winsorization: set extreme values to a threshold).\n",
        "\n",
        "- Transform them (e.g., log-scaling skewed data).\n",
        "\n",
        "- Drop them (if they are clear errors)."
      ],
      "metadata": {
        "id": "ulTFaQMCtWdo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9MVimQbmtZl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing Date/Time Features"
      ],
      "metadata": {
        "id": "tyf1zhq4tZru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For datetime columns, I:\n",
        "\n",
        "- Convert to proper datetime format.\n",
        "\n",
        "- Extract useful parts (year, month, day, weekday, hour).\n",
        "\n",
        "- Calculate differences (e.g., time since event, days until deadline).\n",
        "\n",
        "- Align time zones if necessary."
      ],
      "metadata": {
        "id": "F3K5Jqi0tfu3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "orvFsQrEtjQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text & String Cleaning"
      ],
      "metadata": {
        "id": "GAIMYGMZtjWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For text columns, I consider:\n",
        "\n",
        "- Stripping whitespace, correcting casing.\n",
        "\n",
        "- Removing special characters or formatting artifacts.\n",
        "\n",
        "- Standardizing categories (e.g., “male” vs “Male” vs “M”).\n",
        "\n",
        "- Handling high-cardinality text separately (embedding, NLP later if relevant)."
      ],
      "metadata": {
        "id": "5g8XGgWHtmk0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DyJNjvEuts08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling & Normalization (Optional at This Stage)"
      ],
      "metadata": {
        "id": "kp8KDULkts5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For numeric features, I may prepare them for modeling:\n",
        "\n",
        "- Standardization (z-score): center at mean = 0, std = 1.\n",
        "\n",
        "- Normalization (min-max): scale to range [0,1].\n",
        "\n",
        "- Log transform: reduce skew for highly right-skewed features.\n",
        "\n",
        ">Some algorithms (e.g., Logistic Regression, SVM, Neural Nets) are sensitive to scale; others (e.g., Decision Trees, Random Forests) are not"
      ],
      "metadata": {
        "id": "yugsDqnCtu52"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kBkfOgmat5Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preventing Data Leakage"
      ],
      "metadata": {
        "id": "1OlG4vYpt5dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I make sure that:\n",
        "\n",
        "- Test/validation sets never use information from training data.\n",
        "\n",
        "- Future information is not included in features for past predictions.\n",
        "\n",
        "- Derived features are calculated consistently across train/test splits."
      ],
      "metadata": {
        "id": "A3698Bemt7QL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vA0nhKI8uCXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Data Cleaning Checklist"
      ],
      "metadata": {
        "id": "9aacfLuOuCoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Identified and handled missing values\n",
        "\n",
        " - ⬜️ Verified column data types\n",
        "\n",
        " - ⬜️ Checked and removed duplicates\n",
        "\n",
        " - ⬜️ Investigated and treated outliers\n",
        "\n",
        " - ⬜️ Standardized date/time columns\n",
        "\n",
        " - ⬜️ Cleaned text and categorical values\n",
        "\n",
        " - ⬜️ Applied scaling/normalization if needed\n",
        "\n",
        " - ⬜️ Checked for potential data leakage"
      ],
      "metadata": {
        "id": "hmAzEApvuFHq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hYmLjOT1uNDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Feature Engineering"
      ],
      "metadata": {
        "id": "MliO47rYuOa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "Once the dataset is clean, I want to enrich it by creating new features that capture important patterns.\n",
        "Sometimes, the raw data alone doesn’t tell the full story — but engineered features can reveal hidden relationships.\n",
        "\n",
        "Feature engineering often makes the difference between a baseline model and a high-performing model.\n",
        "\n",
        ">Models are only as good as the features they’re fed. Feature engineering is my chance to inject domain knowledge into the dataset."
      ],
      "metadata": {
        "id": "npH9a_Esusk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types of Feature Engineering"
      ],
      "metadata": {
        "id": "gj03zT2Buw1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think of feature engineering in several categories:\n",
        "\n",
        "1. **Numeric Transformations**\n",
        "\n",
        "- Log-transform skewed variables (e.g., income, transaction amounts).\n",
        "\n",
        "- Binning continuous variables into categories (e.g., age groups).\n",
        "\n",
        "- Ratios and percentages (e.g., feature_a / feature_b).\n",
        "\n",
        "- Polynomial or interaction terms (e.g., feature_a * feature_b).\n",
        "\n",
        "2. **Categorical Encoding**\n",
        "\n",
        "- One-hot encoding: convert categories into dummy variables.\n",
        "\n",
        "- Label encoding: assign integers (useful for ordinal data).\n",
        "\n",
        "- Frequency encoding: replace categories with their frequency count.\n",
        "\n",
        "- Grouping rare categories: combine small classes into “Other.”\n",
        "\n",
        "3. **Datetime Features**\n",
        "\n",
        "From a single timestamp, I can extract:\n",
        "\n",
        "- Year, month, day, weekday, quarter.\n",
        "\n",
        "- Hour of the day (for time-of-day effects).\n",
        "\n",
        "- Time differences (e.g., days since signup, days until expiration).\n",
        "\n",
        "- Seasonality flags (holiday, weekend, summer vs winter).\n",
        "\n",
        "4. **Text Features**\n",
        "\n",
        "If I have free-text columns:\n",
        "\n",
        "- Length of the text (number of words, characters).\n",
        "\n",
        "- Presence of certain keywords.\n",
        "\n",
        "- Bag-of-words or embeddings (if NLP is relevant).\n",
        "\n",
        "5. **Domain-Specific Features**\n",
        "\n",
        "Depending on the dataset context, I may create:\n",
        "\n",
        "- Risk scores (e.g., credit risk ratio).\n",
        "\n",
        "- Aggregates (e.g., average purchases per customer).\n",
        "\n",
        "- Flags (e.g., “is_high_value_customer” = 1 if spend > threshold).\n",
        "\n",
        ">The best features often come from domain knowledge, not just automatic transformations."
      ],
      "metadata": {
        "id": "0X636E6MvQJw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnhAb9QBvpE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection vs Feature Creation"
      ],
      "metadata": {
        "id": "VPFqJXUbvhUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering is not only about adding new features — it’s also about deciding which features to keep.\n",
        "\n",
        "- Drop irrelevant or redundant features (e.g., unique IDs, duplicates).\n",
        "\n",
        "- Remove highly correlated features to reduce multicollinearity.\n",
        "\n",
        "- Keep features that improve interpretability or model stability."
      ],
      "metadata": {
        "id": "9P_qrrZIwDx4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWlFOf3XwHR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interaction Features"
      ],
      "metadata": {
        "id": "E8rHUyK8wHdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, two variables combined give more insight than separately.\n",
        "\n",
        "- *Example*: `price` * `quantity` = total_spent.\n",
        "\n",
        "- *Example*: `age_group` + `product_type` → segment performance.\n",
        "\n",
        "I always document these combinations so I remember why I created them."
      ],
      "metadata": {
        "id": "FttrIqA5wRN-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1LjqSC7w6Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling High Cardinality"
      ],
      "metadata": {
        "id": "xfHQqdOdw6Vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If a categorical column has hundreds of categories (e.g., cities, product IDs), I plan carefully:\n",
        "\n",
        "- Group rare values into “Other.”\n",
        "\n",
        "- Use frequency encoding.\n",
        "\n",
        "- Consider embeddings for extremely large cardinality."
      ],
      "metadata": {
        "id": "8kovcA7ww9HX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vODMpRYCxvbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Derived Features Log (Template)"
      ],
      "metadata": {
        "id": "SmKHlK7dxvjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| New Feature  | Formula / Transformation             | Rationale                                          |\n",
        "| ------------ | ------------------------------------ | -------------------------------------------------- |\n",
        "| income\\_log  | log(income + 1)                      | Reduce skew and highlight relative differences     |\n",
        "| age\\_group   | bin(age) → {0–18, 19–35, 36–60, 61+} | Easier interpretation, capture non-linear patterns |\n",
        "| days\\_active | today – signup\\_date                 | Measure customer lifetime                          |\n",
        "| ratio\\_ab    | feature\\_a / feature\\_b              | Highlight proportional relationship                |\n"
      ],
      "metadata": {
        "id": "B7Not2ssxx-W"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MIDtvZC_x2JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Feature Engineering Checklist"
      ],
      "metadata": {
        "id": "VSCIMS87x2go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Created transformations for skewed numeric variables\n",
        "\n",
        " - ⬜️ Extracted useful datetime components\n",
        "\n",
        " - ⬜️ Encoded categorical features (one-hot, label, or frequency)\n",
        "\n",
        " - ⬜️ Grouped or flagged rare categories\n",
        "\n",
        " - ⬜️ Designed domain-specific variables\n",
        "\n",
        " - ⬜️ Logged all new features in a feature dictionary"
      ],
      "metadata": {
        "id": "12gx62rIx4kd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wavPzAleyAS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Feature Transformation & Data Splitting"
      ],
      "metadata": {
        "id": "mssgdAn7yAby"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "Even after cleaning and engineering features, the dataset may still not be ready for modeling.\n",
        "Models often expect features in specific formats, and I also need to ensure that I evaluate my model fairly with proper train/test/validation splits.\n",
        "\n",
        "- The purpose of this step is to:\n",
        "\n",
        "- Transform categorical and numeric features into usable formats.\n",
        "\n",
        "- Scale variables where needed.\n",
        "\n",
        "- Encode labels for supervised learning tasks.\n",
        "\n",
        "- Split the dataset into training, validation, and test sets without leakage.\n",
        "\n",
        ">At this stage, I am building the bridge between raw/engineered data and the algorithms that will learn from it."
      ],
      "metadata": {
        "id": "u2ZPe7Y3yGbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical Variables"
      ],
      "metadata": {
        "id": "f2kHz7HRySVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different algorithms require categorical features in numeric form.\n",
        "\n",
        "- One-Hot Encoding (OHE):\n",
        "\n",
        "   - Each category becomes its own column (0/1 flag).\n",
        "\n",
        "   - Best for tree-based models (Decision Trees, Random Forests, XGBoost).\n",
        "\n",
        "   - Problem: high-dimensionality if too many categories.\n",
        "\n",
        "- Label Encoding:\n",
        "\n",
        "   - Assigns an integer value to each category.\n",
        "\n",
        "   - Works well with ordinal features (e.g., “low, medium, high”).\n",
        "\n",
        "   - Risk: for non-ordinal features, models may assume false order.\n",
        "\n",
        "- Frequency/Count Encoding:\n",
        "\n",
        "   - Replace categories with their frequency or counts.\n",
        "\n",
        "   - Useful for high-cardinality variables.\n",
        "\n",
        "- Target/Mean Encoding:\n",
        "\n",
        "   - Replace categories with average target rate.\n",
        "\n",
        "   - Can be powerful, but risky (must avoid leakage)."
      ],
      "metadata": {
        "id": "B3QDZJEkyUnb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z8wtoSM2y8pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling Numeric Features"
      ],
      "metadata": {
        "id": "_GEi6Ny8y8-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some algorithms are sensitive to feature scales (e.g., Logistic Regression, SVM, Neural Networks).\n",
        "Others (tree-based models) are scale-invariant.\n",
        "\n",
        "- Standardization (Z-score): `(x – mean) / std` → mean = 0, std = 1.\n",
        "\n",
        "- Normalization (Min-Max): scales values into `[0,1]`.\n",
        "\n",
        "- Log Transform: reduces skew in highly right-skewed features."
      ],
      "metadata": {
        "id": "ffyquWUEzF7F"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cv5jTiFG1CFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the Target Variable"
      ],
      "metadata": {
        "id": "00lkcTqY1CRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For binary classification: encode target as {0,1}.\n",
        "\n",
        "- For multi-class classification: integer labels or one-hot vectors.\n",
        "\n",
        "- For regression: keep as continuous numeric values."
      ],
      "metadata": {
        "id": "_EMlYA001N_x"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkysOdpA1Si9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train / Validation / Test Splitting"
      ],
      "metadata": {
        "id": "l4chuvI81Snq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To properly evaluate performance, I split my dataset into subsets:\n",
        "\n",
        "- Training set: the portion used to train the model (usually 60–70%).\n",
        "\n",
        "- Validation set: used to tune hyperparameters and compare models (15–20%).\n",
        "\n",
        "- Test set: final unseen data to evaluate real-world performance (15–20%).\n",
        "\n",
        "### Important considerations:\n",
        "\n",
        "- Use stratified sampling for classification if classes are imbalanced.\n",
        "\n",
        "- For time-series problems, split chronologically (train on past, test on future).\n",
        "\n",
        "- Ensure no data leakage (the same person/item/event shouldn’t appear in both train and test)."
      ],
      "metadata": {
        "id": "SgEKxSCZ1UvB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QsMVowR61gK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation (Optional but Recommended)"
      ],
      "metadata": {
        "id": "HUvzFCiS1gQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of one validation split, I may use k-fold cross-validation:\n",
        "\n",
        "- The training data is split into k folds (e.g., 5).\n",
        "\n",
        "- The model trains k times, each time using one fold as validation and the rest as training.\n",
        "\n",
        "- The average score across folds gives a more reliable estimate.\n",
        "\n",
        ">This is especially useful for small datasets where I want to maximize training data."
      ],
      "metadata": {
        "id": "AUtOOJ8D1ntS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vx1ijaZ21roZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Transformation & Splitting Checklist"
      ],
      "metadata": {
        "id": "7GEZCSAC1rxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Encoded categorical variables properly\n",
        "\n",
        " - ✅ Scaled/normalized numeric features if required\n",
        "\n",
        " - ⬜️ Encoded the target variable consistently\n",
        "\n",
        " - ⬜️ Split dataset into train/validation/test\n",
        "\n",
        " - ⬜️ Used stratification or time-based splits if needed\n",
        "\n",
        " - ⬜️ Considered cross-validation for stability\n",
        "\n",
        " - ⬜️ Verified no data leakage between splits"
      ],
      "metadata": {
        "id": "MPUmeuXc1tld"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xzg1_nqr11R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Model Selection & Baseline Modeling"
      ],
      "metadata": {
        "id": "45HW9U4L11dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "Now that my dataset is clean, engineered, and split, I need to select candidate models to try.\n",
        "The purpose of this step is twofold:\n",
        "\n",
        "1. Establish a baseline model to measure progress against.\n",
        "\n",
        "2. Compare different algorithms that might suit my dataset.\n",
        "\n",
        ">A baseline doesn’t need to be perfect — it’s simply a starting point. If a complex model can’t beat the baseline, it’s probably not worth the extra effort."
      ],
      "metadata": {
        "id": "78Srk6nx2Cuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is a Baseline Model?"
      ],
      "metadata": {
        "id": "G5kw-7oQ2L0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A baseline model is a simple first attempt that sets expectations.\n",
        "Examples include:\n",
        "\n",
        "- For classification problems:\n",
        "\n",
        "   - Predict the most frequent class for all rows.\n",
        "\n",
        "   - Logistic Regression with no hyperparameter tuning.\n",
        "\n",
        "- For regression problems:\n",
        "\n",
        "   - Always predict the mean or median.\n",
        "\n",
        "   - Linear Regression with no feature scaling tweaks."
      ],
      "metadata": {
        "id": "YKmQ2sHy2TFF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "honS9F9Z2XAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Families to Consider"
      ],
      "metadata": {
        "id": "h-fnND3S26ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since I want to keep this template reusable, I group models into families:\n",
        "\n",
        "🔹 Linear Models\n",
        "\n",
        "- **Logistic Regression (classification)**\n",
        "\n",
        "- **Linear Regression (regression)**\n",
        "\n",
        "- Pros: simple, interpretable, fast.\n",
        "\n",
        "- Cons: struggles with non-linear relationships.\n",
        "\n",
        "🔹 Tree-Based Models\n",
        "\n",
        "- **Decision Trees**\n",
        "\n",
        "- **Random Forests**\n",
        "\n",
        "- Gradient Boosted Trees (XGBoost, LightGBM, CatBoost)\n",
        "\n",
        "- Pros: handle categorical variables, non-linearities, interactions.\n",
        "\n",
        "- Cons: can overfit without proper tuning.\n",
        "\n",
        "🔹 Distance-Based Models\n",
        "\n",
        "- **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "- Pros: intuitive, works well with small datasets.\n",
        "\n",
        "- Cons: slow with large datasets, sensitive to scaling.\n",
        "\n",
        "🔹 Margin-Based Models\n",
        "\n",
        "- **Support Vector Machines (SVM)**\n",
        "\n",
        "- Pros: effective with clear class separation.\n",
        "\n",
        "- Cons: slow on large datasets, requires scaling.\n",
        "\n",
        "🔹 Neural Networks\n",
        "\n",
        "- **Feedforward Neural Nets (basic deep learning)**\n",
        "\n",
        "- Pros: flexible, powerful on large complex data.\n",
        "\n",
        "- Cons: requires more data, longer training, less interpretable."
      ],
      "metadata": {
        "id": "Ba7XrXil2mM9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9n-Uwa7d24Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How I Will Compare Models"
      ],
      "metadata": {
        "id": "7Gafju3828uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I plan to:\n",
        "\n",
        "1. Train multiple algorithms using default parameters.\n",
        "\n",
        "2. Evaluate each on the validation set using the metric I chose in Step 1.\n",
        "\n",
        "3. Record results in a comparison table.\n",
        "\n",
        "4. Select the most promising model(s) for tuning in the next step.\n",
        "\n",
        "| Model               | Validation Accuracy | Notes                                  |\n",
        "| ------------------- | ------------------- | -------------------------------------- |\n",
        "| Majority Class      | 0.60                | Baseline (predict most frequent class) |\n",
        "| Logistic Regression | 0.72                | Simple linear model                    |\n",
        "| Decision Tree       | 0.75                | Slight overfitting risk                |\n",
        "| Random Forest       | 0.80                | Promising, stable                      |\n",
        "| XGBoost             | 0.82                | Strong candidate                       |\n"
      ],
      "metadata": {
        "id": "ICgquLzS3D77"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7AlBOJz_3LMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preventing Overfitting Early"
      ],
      "metadata": {
        "id": "7VdoxIVf3LYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even in baseline tests, I keep in mind:\n",
        "\n",
        "- High training score but low validation score = overfitting.\n",
        "\n",
        "- Similar training/validation scores = healthy baseline.\n",
        "\n",
        "I don’t tune too much yet — the goal is broad comparison, not perfection."
      ],
      "metadata": {
        "id": "-OEuaM_k3Nv9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fA3wjJ33WUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Model Selection Checklist"
      ],
      "metadata": {
        "id": "nx_kAcef3Xyq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ✅ Built a simple baseline model\n",
        "\n",
        "- ✅ Selected a diverse set of algorithms to compare\n",
        "\n",
        "- ⬜️ Evaluated each model with the chosen metric\n",
        "\n",
        "- ⬜️ Recorded results in a comparison table\n",
        "\n",
        "- ⬜️ Identified at least one strong candidate for tuning\n",
        "\n",
        "- ⬜️ Checked for early signs of overfitting"
      ],
      "metadata": {
        "id": "Bak7NTE03aNX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qclJlhCR3mW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Model Training & Evaluation"
      ],
      "metadata": {
        "id": "7LzzeRrj3mhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "After building baselines and shortlisting candidate models, I now want to:\n",
        "\n",
        "- Train them more carefully (not just defaults).\n",
        "\n",
        "- Evaluate their performance on the validation set.\n",
        "\n",
        "- Compare results across multiple metrics, not just one.\n",
        "\n",
        "- Understand where each model performs well or poorly.\n",
        "\n",
        ">This step helps me figure out which models are truly promising before moving to tuning."
      ],
      "metadata": {
        "id": "WVcP_4rp4Hib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Strategy"
      ],
      "metadata": {
        "id": "DBPlEHWH4P7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training my models, I keep in mind:\n",
        "\n",
        "- Consistency: use the same train/validation split (or cross-validation) for fair comparison.\n",
        "\n",
        "- Reproducibility: set random seeds so results are stable.\n",
        "\n",
        "- Efficiency: start small, then increase complexity if needed.\n",
        "\n",
        "I remind myself: more complex ≠ always better. Sometimes simple models outperform heavy ones."
      ],
      "metadata": {
        "id": "3ZXWH6VA4Yke"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bqKSfNvW4aZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "LS4WtWjU4a4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I already defined my primary metric in Step 1 (e.g., Accuracy, RMSE).\n",
        "Now I also look at secondary metrics to get a fuller picture.\n",
        "\n",
        "**For classification problems:**\n",
        "\n",
        "- Accuracy (overall correctness)\n",
        "\n",
        "- Precision (how many predicted positives are actually positive)\n",
        "\n",
        "- Recall (how many actual positives I found)\n",
        "\n",
        "- F1-score (balance of precision & recall)\n",
        "\n",
        "- ROC-AUC (ranking ability, threshold-independent)\n",
        "\n",
        "**For regression problems:**\n",
        "\n",
        "- RMSE (root mean squared error)\n",
        "\n",
        "- MAE (mean absolute error)\n",
        "\n",
        "- R² (variance explained)"
      ],
      "metadata": {
        "id": "pCQmLeOc4dKv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CuVMBJYj4k2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Comparison Table"
      ],
      "metadata": {
        "id": "cDVJLYr24k6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I record each model’s performance side by side.\n",
        "\n",
        "| Model               | Accuracy | Precision | Recall | F1-score | ROC-AUC | Notes                 |\n",
        "| ------------------- | -------- | --------- | ------ | -------- | ------- | --------------------- |\n",
        "| Logistic Regression | 0.72     | 0.70      | 0.65   | 0.67     | 0.74    | Simple, interpretable |\n",
        "| Decision Tree       | 0.75     | 0.73      | 0.70   | 0.71     | 0.76    | Tends to overfit      |\n",
        "| Random Forest       | 0.80     | 0.78      | 0.76   | 0.77     | 0.84    | Robust, balanced      |\n",
        "| XGBoost             | 0.82     | 0.80      | 0.78   | 0.79     | 0.86    | Best so far           |\n"
      ],
      "metadata": {
        "id": "jR8Vhhic4oGB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I64cw2Oj4tBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias vs Variance Check"
      ],
      "metadata": {
        "id": "klbkRT8C4tGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I compare training vs validation scores:\n",
        "\n",
        "- If training >> validation → overfitting.\n",
        "\n",
        "- If training ≈ validation but both low → underfitting.\n",
        "\n",
        "- If both are high → model generalizes well.\n",
        "\n",
        "This helps me know whether to simplify the model or add complexity."
      ],
      "metadata": {
        "id": "FBo5Huos4uQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TPBqjPK40Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Analysis"
      ],
      "metadata": {
        "id": "t-Ce-TDT40LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of just looking at overall accuracy, I also ask:\n",
        "\n",
        "- Which cases are most often misclassified?\n",
        "\n",
        "- Do certain categories get worse predictions than others?\n",
        "\n",
        "- Are errors random or systematic (bias)?\n",
        "\n",
        "Error analysis gives me insight into what to fix (feature engineering, resampling, better model)."
      ],
      "metadata": {
        "id": "y_QMz51D43nO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LRhWE9-545hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Training & Evaluation Checklist"
      ],
      "metadata": {
        "id": "d00iL_cF45lx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Used consistent splits for all models\n",
        "\n",
        " - ⬜️ Trained multiple models with same preprocessing pipeline\n",
        "\n",
        " - ⬜️ Evaluated with both primary and secondary metrics\n",
        "\n",
        " - ⬜️ Compared training vs validation performance (bias/variance)\n",
        "\n",
        " - ⬜️ Logged results in a model comparison table\n",
        "\n",
        " - ⬜️ Performed basic error analysis"
      ],
      "metadata": {
        "id": "3XFDFCAK48IY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12MW137v5kZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 10: Hyperparameter Tuning & Cross-Validation"
      ],
      "metadata": {
        "id": "p5mkA06q5knR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why I Am Doing This**\n",
        "\n",
        "My baseline and initial model comparisons (Steps 8–9) tell me what works; now I want to squeeze out reliable performance without fooling myself. This step is about:\n",
        "\n",
        "- Systematically searching hyperparameters.\n",
        "\n",
        "- Using robust cross-validation to estimate performance.\n",
        "\n",
        "- Avoiding leakage and overfitting to the validation set.\n",
        "\n",
        "- Selecting a configuration that is accurate, stable, and reproducible.\n",
        "\n",
        ">Goal: pick a model + hyperparameters that generalize, not just look good on one lucky split."
      ],
      "metadata": {
        "id": "EVYJpkhm52qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What I Tune (Scope)"
      ],
      "metadata": {
        "id": "BJ2tBuMN59hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I consider tuning both algorithm hyperparameters and pipeline choices:\n",
        "\n",
        "- Algorithm hyperparameters (e.g., tree depth, regularization strength, learning rate).\n",
        "\n",
        "- Preprocessing knobs (e.g., scaler type, imputation strategy).\n",
        "\n",
        "- Class imbalance handling (e.g., class weights, sampling ratios).\n",
        "\n",
        "- Decision threshold (for classification, tune threshold after model training).\n",
        "\n",
        "I keep the search space realistic (broad enough to discover good regions, narrow enough to finish in time)."
      ],
      "metadata": {
        "id": "b3xmaqTT6CEt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLZ6x9B06Fym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-Validation Strategies (Picking the Right One)"
      ],
      "metadata": {
        "id": "0tfSH7xP6F7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose a CV scheme that matches my data:\n",
        "\n",
        "- K-Fold CV (k=5 or 10): default for balanced, IID data.\n",
        "\n",
        "- Stratified K-Fold: preserves class ratios (my default for classification).\n",
        "\n",
        "- Group K-Fold: ensures entire groups (e.g., users, stores) don’t leak across folds.\n",
        "\n",
        "- TimeSeriesSplit (rolling/forward chaining): train on past → validate on future (no time leakage).\n",
        "\n",
        "- Repeated K-Fold: repeats folds to reduce variance if the dataset is small.\n",
        "\n",
        "- Nested CV (outer + inner loops): gold standard to avoid optimistic bias when model selection itself is tuned; used when I need an unbiased estimate of the tuned pipeline.\n",
        "\n",
        ">⚠️ If there are groups, sessions, or entities that can repeat, I must keep them within a fold to prevent leakage."
      ],
      "metadata": {
        "id": "tqPjtPuf6H_B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wZOMjCDS6RRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Search Methods (How I Explore the Space)"
      ],
      "metadata": {
        "id": "39ioZIuC6R31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Manual / heuristic search: quick sanity sweeps for a new dataset.\n",
        "\n",
        "- Grid Search: exhaustive over a small, curated grid (costly; good for a few parameters).\n",
        "\n",
        "- Random Search: broad, cheap exploration; surprisingly effective for high-dimensional spaces.\n",
        "\n",
        "- Bayesian Optimization (conceptual): iteratively proposes promising configs (e.g., TPE/GP ideas); efficient for expensive models.\n",
        "\n",
        "- Successive Halving / Hyperband (conceptual): allocate more budget to winners, early-stop losers.\n",
        "\n",
        "- Early Stopping (for boosted trees / neural nets): stop training when validation metric stops improving.\n",
        "\n",
        ">Rule of thumb: start with Random Search to find good regions, then Grid or Bayesian to refine."
      ],
      "metadata": {
        "id": "Q_xKJ5Hv6UmK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EhTTyFs6gMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Typical Hyperparameters (Cheat Sheet)"
      ],
      "metadata": {
        "id": "cVY-A3rS6gRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I tailor ranges to dataset size and compute budget; ranges below are starting points.\n",
        "\n",
        "**Linear / Logistic Regression**\n",
        "\n",
        "- `C` (inverse regularization): log-uniform ~ `[1e-3, 1e+3]`\n",
        "\n",
        "- `penalty`: `l2` (often best default)\n",
        "\n",
        "- `class_weight`: `None` or `balanced` (for imbalance)\n",
        "\n",
        "- Notes: scale features; watch multicollinearity.\n",
        "\n",
        "**SVM (Classification)**\n",
        "\n",
        "- `kernel`: `linear` or `rbf`\n",
        "\n",
        "- `C` log-uniform `[1e-3, 1e+3]`\n",
        "\n",
        "- `gamma` (RBF): log-uniform `[1e-4, 1e+1]`\n",
        "\n",
        "- Notes: scale features; sensitive to C/gamma.\n",
        "\n",
        "**K-Nearest Neighbors**\n",
        "\n",
        "- `n_neighbors`: `[3, 5, 7, 9, 15, 25]`\n",
        "\n",
        "- `weights`: `uniform` vs `distance`\n",
        "\n",
        "- `p`: `1` (Manhattan) or `2` (Euclidean)\n",
        "\n",
        "- Notes: scale features; costly at inference.\n",
        "\n",
        "**Decision Tree**\n",
        "\n",
        "- `max_depth`: `[3, 5, 7, 10, None]`\n",
        "\n",
        "- `min_samples_leaf`: `[1, 2, 5, 10]`\n",
        "\n",
        "- `min_samples_split`: `[2, 5, 10]`\n",
        "\n",
        "- `max_features`: `None`, `sqrt`, or `fraction`\n",
        "\n",
        "- `ccp_alpha`: pruning `[0.0, 0.01, 0.05]`\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "- `n_estimators`: `[200, 500, 1000]`\n",
        "\n",
        "- `max_depth`: `[None, 10, 20, 30]`\n",
        "\n",
        "- `min_samples_leaf`: `[1, 2, 5]`\n",
        "\n",
        "- `max_features`: `sqrt`, `log2`, `fraction`\n",
        "\n",
        "- `bootstrap`: `True/False`\n",
        "\n",
        "- `class_weight`: `None/balanced`\n",
        "\n",
        "**Gradient-Boosted Trees (generic / XGBoost-like)**\n",
        "\n",
        "- `n_estimators`: `[200, 500, 1000]` (with early stopping)\n",
        "\n",
        "- `learning_rate`: `[0.01, 0.1, 0.2]`\n",
        "\n",
        "- `max_depth`: `[2, 3, 5, 7]`\n",
        "\n",
        "- `subsample`: `[0.6, 0.8, 1.0]`\n",
        "\n",
        "- `colsample_bytree`: `[0.6, 0.8, 1.0]`\n",
        "\n",
        "- `min_child_weight` / `min_samples_leaf`: `[1, 3, 5]`\n",
        "\n",
        "- `reg_lambda` / `reg_alpha`: `[0, 1, 10]`\n",
        "\n",
        "**Neural Network (MLP-style)**\n",
        "\n",
        "- `hidden_layers`: e.g., `[(64,), (128,), (128,64)]`\n",
        "\n",
        "- `activation`: `relu`, `tanh`\n",
        "\n",
        "- `alpha` (L2): `[1e-5, 1e-3, 1e-1]`\n",
        "\n",
        "- `learning_rate_init`: `[1e-4, 1e-3, 1e-2]`\n",
        "\n",
        "- `batch_size`: `[32, 64, 128]`\n",
        "\n",
        "- `epochs`: budget-constrained with early stopping\n",
        "\n",
        "- Notes: scale features; consider dropout (conceptually) and patience."
      ],
      "metadata": {
        "id": "GSu-rVt960YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdjtBy8C8Afz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imbalance & Threshold Tuning"
      ],
      "metadata": {
        "id": "1qZtm-NX8kLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If classes are imbalanced or costs are asymmetric, I:\n",
        "\n",
        "- Use Stratified CV, consider class_weight or resampling (undersample/oversample).\n",
        "\n",
        "- Optimize thresholds on validation data to maximize target metric (e.g., F1, Youden’s J, cost-sensitive utility).\n",
        "\n",
        "- Consider probability calibration (Platt/Isotonic) if calibrated probabilities matter.\n",
        "\n",
        ">Important: threshold tuning is done after model fitting, using validation predictions only."
      ],
      "metadata": {
        "id": "G3jwtlxv8k95"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJtHRAWR8o0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curves & Validation Curves"
      ],
      "metadata": {
        "id": "1Q2xvJic8o4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Learning curves: train size vs. score → diagnose under/overfitting and whether more data helps.\n",
        "\n",
        "- Validation curves: metric vs. a single hyperparameter → find sweet spots (e.g., depth, C).\n",
        "\n",
        "These plots guide where to expand or tighten the search."
      ],
      "metadata": {
        "id": "JBSw_TjP8rET"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZYnNlGtQ8uKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Design (Templates)"
      ],
      "metadata": {
        "id": "SjmlpuY78uRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search Space Log (Template)\n",
        "\n",
        "| Param             | Values / Distribution | Rationale           |\n",
        "| ----------------- | --------------------- | ------------------- |\n",
        "| max\\_depth        | \\[3, 5, 7, 10]        | control complexity  |\n",
        "| n\\_estimators     | \\[200, 500, 1000]     | stability vs time   |\n",
        "| learning\\_rate    | \\[0.01, 0.1]          | trade speed/overfit |\n",
        "| subsample         | \\[0.6, 0.8, 1.0]      | reduce variance     |\n",
        "| colsample\\_bytree | \\[0.6, 0.8, 1.0]      | reduce correlation  |\n",
        "\n",
        "Experiment Log (Template)\n",
        "\n",
        "| Exp ID | Model    | CV Scheme     | Mean (Primary) | Std   | Sec. Metric  | Fit Time | Notes         |\n",
        "| -----: | -------- | ------------- | -------------- | ----- | ------------ | -------: | ------------- |\n",
        "|    001 | Logistic | StratKFold(5) | 0.742          | 0.009 | ROC-AUC 0.80 |     0:07 | baseline      |\n",
        "|    014 | RF       | StratKFold(5) | 0.802          | 0.011 | ROC-AUC 0.87 |     1:23 | depth=20      |\n",
        "|    027 | GBT      | StratKFold(5) | **0.824**      | 0.008 | ROC-AUC 0.90 |     1:56 | early stop=50 |\n"
      ],
      "metadata": {
        "id": "D5NbBSLn8wSS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D5vhnHm89ERW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leakage Prevention & Reproducibility"
      ],
      "metadata": {
        "id": "--Bcrbch9Ed7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Wrap preprocessing + model in a single pipeline so CV never sees training stats from validation folds.\n",
        "\n",
        "- For time-dependent data, use TimeSeriesSplit and compute all features from the past only.\n",
        "\n",
        "- Fix random seeds where possible; note library versions and hardware.\n",
        "\n",
        "- Keep train/val/test separation sacred; never peek at test."
      ],
      "metadata": {
        "id": "Gaii7GIZ9HBm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0ez-D5c9Jws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection Criteria (When I Stop)"
      ],
      "metadata": {
        "id": "EftkbuGe9KAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I pick the configuration that balances:\n",
        "\n",
        "- Primary metric (best mean CV).\n",
        "\n",
        "- Stability (small CV std).\n",
        "\n",
        "- Simplicity (prefer fewer knobs if performance is tied).\n",
        "\n",
        "- Inference cost (latency/memory).\n",
        "\n",
        "- Fairness / calibration (if relevant).\n",
        "\n",
        "If two configs tie, I choose the simpler, faster, or more interpretable one."
      ],
      "metadata": {
        "id": "iOWTJnmc9PEi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHLDkUbh9SAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My Tuning & CV Checklist"
      ],
      "metadata": {
        "id": "HL49J0Hd9SF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - ✅ Picked an appropriate CV strategy (Stratified / Group / TimeSeries)\n",
        "\n",
        " - ⬜️ Defined a realistic search space\n",
        "\n",
        " - ⬜️ Chose a search method (Random → Grid / Bayesian)\n",
        "\n",
        " - ⬜️ Used pipelines to avoid leakage\n",
        "\n",
        " - ⬜️ Logged mean ± std across folds\n",
        "\n",
        " - ⬜️ Considered class imbalance and threshold tuning\n",
        "\n",
        " - ⬜️ Checked learning/validation curves\n",
        "\n",
        " - ⬜️ Fixed seeds & documented environment\n",
        "\n",
        " - ⬜️ Selected a final configuration based on metric + stability"
      ],
      "metadata": {
        "id": "fmHW_CoU9UM1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cv_O65Ds9eWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 11:"
      ],
      "metadata": {
        "id": "D3GKm6LI9eez"
      }
    }
  ]
}